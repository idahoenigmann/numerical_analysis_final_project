\documentclass[]{article}

\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage[margin=3cm]{geometry}
\usepackage{enumitem}
\usepackage{amsthm}


\renewcommand*{\proofname}{Beweis}

%opening
\title{Abschlussprojekt}
\author{Ida Hönigmann \and Fabian Dopf}

\begin{document}

\maketitle

\section*{Aufgabe 1: Aufwandsordnung numerischer Verfahren}
Wir betrachten ein abstraktes numerisches Verfahren, das für $N \in \mathbb{N}$ Eingabedaten eine Laufzeit von $y_N \in \mathbb{R}_+$ hat. Man sagt, das Verfahren habe Aufwandsordnung $p > 0$, falls eine Konstante $C > 0$ existiert, sodass $y_N \leq C N^p$ für alle $N \in \mathbb{N}$.

\subsection*{Teilaufgabe 1a:}
Die Aufwandsordnung lässt sich über die Folge $\{p_N\}_{N \in \mathbb{N}}$ mit

\begin{align}
	\label{eq:def_N}
	p_N = \frac{\log(y_{2N})-\log(y_n)}{\log(2)} \text{ für } N \in \mathbb{N}
\end{align}

quantifizieren. Beachten Sie, dass die Bestimmung von $p_N$ die Verfügbarkeit von zwei aufeinanderfolgenden Folgengliedern $y_N$ und $y_{2N}$ erfordert. Verwenden Sie den Ansatz $y_N = CN^p$ und leiten Sie die Formel in \ref{eq:def_N} her!

\begin{proof}
	Annahme: $\forall N \in \mathbb{N}$ ist $p_N$, sodass $y_N \leq C N^{p_N}$ für ein $C > 0$.
	
	Für ein beliebiges $N \in \mathbb{N}$ gilt $\exists C_{1N}, C_{2N} > 0$ und $p_{1N}, p_{2N} > 0$ mit $y_N \leq C_{1N} N^{p_{1N}}$ und $y_{2N} \leq C_{2N} (2N)^{p_{2N}}$.
	
	Für $C:=max\{C_{1N}, C_{2N}\}$ und $p_N:=max\{p_{1N}, p_{2N}\}$ gilt $y_N \leq C_{1N} N^{p_{1N}} \leq C N^{p_N}$ und $y_{2N} \leq C_{2N} (2N)^{p_{2N}} \leq C (2N)^{p_N}$.
	 
	\begin{align}
		\log(y_{2N}) - \log(y_N) = \log\left(\frac{y_{2N}}{y_N}\right) = \log\left(\frac{C(2N)^{p_N}}{C\cdot N^{p_N}}\right) = \log(2^{p_N}) = p_N \log(2) \\
		\implies p_N = \frac{\log(y_{2N}) - \log(y_N)}{\log(2)}
	\end{align}

	TODO ob das so alles passt...

\end{proof}

\subsection*{Teilaufgabe 1b:}
Sei $\{\delta_N\}_{N \in \mathbb{N}} \subseteq \mathbb{R}$ eine Nullfolge, d.h. es gilt $\delta_N \rightarrow 0$ für $N \rightarrow \infty$. Weiters verhalte sich die Laufzeit wie $y_N = (C+\delta_N)N^p$ mit $C > 0$. Zeigen Sie, dass die Folge $\{p_N\}_{N \in \mathbb{N}}$ gegen $p$ konvergiert, d.h. es gilt $p_N \rightarrow p$ für $N \rightarrow \infty$.

\begin{proof}	
	Zuerst berechnen wir einen Grenzwert, den wir in späterer Folge verwenden werden. Die Gleichungen stimmen, da $\lim$ stetig ist und da laut Voraussetzung $\delta_N$ und somit auch $\delta_{2N}$ als Teilfolge, gegen $0$ konvergieren.
	
	\begin{align}
		\lim\limits_{n\rightarrow\infty} \log\left(\frac{C+\delta_{2N}}{C+\delta_N}\right) = \log\left(\lim\limits_{n\rightarrow\infty}\frac{C+\delta_{2N}}{C+\delta_N}\right) = \log\left(\frac{\lim\limits_{n\rightarrow\infty}C+\delta_{2N}}{\lim\limits_{n\rightarrow\infty}C+\delta_N}\right) = \log\left(\frac{C}{C}\right) = \log(1) = 0
	\end{align}

	Wir berechnen $\lim\limits_{n\rightarrow\infty}p_n$ indem wir die Gleichung \ref{eq:def_N} verwenden. Durch Einsetzen von $y_N = (C+\delta_N)N^p$ und den Rechenregeln von Limiten und dem Logarithmus erhalten wir folgendes:

	\begin{align}
		\implies
		p_N = \frac{\log(y_{2N}) - \log(y_N)}{\log(2)} = \frac{\log((C+\delta_{2N})(2N)^p) - \log((C+\delta_N)N^p)}{\log(2)} = \frac{\log\left(\frac{(C+\delta_{2N})(2N)^p}{(C+\delta_N)N^p}\right)}{\log(2)} \\
		= \frac{\log\left(\frac{(C+\delta_{2N})2^p}{(C+\delta_N)}\right)}{\log(2)} = \frac{p \log(2) + \log\left(\frac{C+\delta_{2N}}{C+\delta_N}\right)}{\log(2)} = p + \frac{\log\left(\frac{C+\delta_{2N}}{C+\delta_N}\right)}{\log(2)} \xrightarrow{n\rightarrow\infty} p + 0 = p
	\end{align}
	
	Zusammenfassend gilt nun $\lim\limits_{n\rightarrow\infty}p_n = p$, was zu zeigen war.

\end{proof}

\subsection*{Teilaufgabe 1c:}
In sogenannter doppelt logarithmischer Darstellung (log-log Plots) wird für beide Koordinatenachsen eine logarithmische Skalierung verwendet, d.h. sowohl die waagrechte als auch die senkrechte Koordinatenachse wird logarithmisch unterteilt. Wie werden Potenzfunktionen der Form $y = c x^p$ in einem log-log Plot dargestellt? Wie können Sie die Ordnung $p$ und die Konstante $c > 0$ aus einem log-log Plot von $y = cx^p$ direkt auslesen?

Darstellung ist Gerade. $c = f(1)$ und $p$ ist Steigung, wenn beide Achsen ''gleich'' skaliert.

\section*{Aufgabe 2: Cholesky-Verfahren und Skyline-Matrizen}
Eine Matrix $A \in \mathbb{R}^{n\times n}$ heißt Skyline-Matrix, falls es für $l = 1, ..., n$ Zahlen $p_l, q_l \in \mathbb{N}_0$ gibt, sodass für die $i$-te Zeile und $j$-te Spalte von $A$ gilt:

\begin{itemize}
	\item $A_{i,k} = 0 \text{ für } k < i - p_i$,
	\item $A_{k,j} = 0 \text{ für } k < j - q_j$,
\end{itemize}

Folgendes Beispiel illustriert diese Aussage:

\begin{align*}
	A = \begin{pmatrix}
		1 &   &   &   & 1 \\
		  & 1 &   & 2 & 2 \\
		  &   & 1 & 3 & 3 \\
		1 & 2 & 3 &14 &18 \\
		  & 4 & 5 &29 &48 \\
	\end{pmatrix}.
\end{align*}

\subsection*{Teilaufgabe 2a:}
Beweisen Sie, dass das Cholesky-Verfahren genau dann wohldefiniert ist (d.h. es wird nicht durch Null dividiert oder die Wurzel aus einer negativen Zahl gezogen), wenn die Matrix $A \in \mathbb{R}^{n\times n}$ symmetrisch und positiv definit ist.

\begin{proof}
	Wir wiederholen zuerst die Definitionen von symmetrischen und positiv definitiven Matrizen.
	
	Eine Matrix $A \in \mathbb{R}^{n\times n}$ heißt symmetrisch, falls $A = A^T$.
	
	Eine Matrix $A \in \mathbb{R}^{n\times n}$ heißt positiv definit, falls $\forall u \in \mathbb{R}^n\setminus\{0\}: u^T A u > 0$.
	
	Wir zeigen $A \in \mathbb{K}^{n\times n}$ symmetrisch und positiv definit impliziert $\exists L \in \mathbb{R}^{n\times n}$ untere Dreiecksmatrix mit $LL^T=A$ durch vollständige Induktion nach $n$.
	
	\begin{itemize}
		\item \textbf{Induktionsanfang:} $n=1$
		
		Wenn $A := \begin{pmatrix}
			a_{11}
		\end{pmatrix} \in \mathbb{R}^{1\times 1}$ eine symmetrische und positiv definite Matrix ist, folgt aus positiv definit, dass für
	
		\begin{align*}
			u := \begin{pmatrix}
				1
			\end{pmatrix} \in \mathbb{R}^1 \setminus\{0\} && 0 < u^TAu = \begin{pmatrix}
				1
			\end{pmatrix} \begin{pmatrix}
				a_{11}
			\end{pmatrix} \begin{pmatrix}
				1
			\end{pmatrix} = a_{11}
		\end{align*}
	
		Definieren wir nun $L := \begin{pmatrix}
			\sqrt{a_{11}}
		\end{pmatrix} \in \mathbb{R}^{1}$, dann gilt
	
		\begin{align*}
			LL^T = \begin{pmatrix}
				\sqrt{a_{11}}
			\end{pmatrix} \begin{pmatrix}
				\sqrt{a_{11}}
			\end{pmatrix} = \begin{pmatrix}
				a_{11}
			\end{pmatrix} = A
		\end{align*}
	
		\item \textbf{Induktionsvoraussetzung:} $\forall A \in \mathbb{R}^{n \times n}$ symmetrisch und positiv definit gilt $\exists L \in \mathbb{R}^{n\times n}$ untere Dreiecksmatrix mit $LL^T=A$.
		
		\item \textbf{Induktionsschritt:} $n-1 \implies n$
		
		Sei $A \in \mathbb{R}^{n\times n}$ eine symmetrische und positiv definite Matrix. Wir definieren eine Matrix $B \in \mathbb{R}^{(n-1)\times(n-1)}$ durch $B_{i,j} := A_{i,j}$, einen Vektor $a \in \mathbb{R}^{n-1}$ durch $a_i := A_{i,n}$ und eine Zahl $\alpha \in \mathbb{R}$ durch $\alpha := A_{n,n}$.
		
		Zusammengefasst gilt nun
		
		\begin{align*}
			A = \begin{pmatrix}
				B & a \\
				a^T & \alpha
			\end{pmatrix} && \text{ wobei sich das }a^T\text{ aus der Symmetrie von }A\text{ ergibt.}
		\end{align*}
	
		Für $B$ gilt, dass es sich um eine symmetrische und positiv definite Matrix aus $\mathbb{R}^{(n-1)\times(n-1)}$ handelt. Laut Induktionsvoraussetzung existiert dazu eine untere Dreiecksmatrix $P \in \mathbb{R}^{(n-1)\times(n-1)}$ mit $PP^T=B$.
		
		Da $B$ positiv definit ist und somit regulär ist, folgt die eindeutige Existenz eines Vektors $l \in \mathbb{R}^{n-1}$ der die Gleichung $Pl=a$ erfüllt.
		
		Wir wollen nun $\beta \in \mathbb{R}$ so definieren, dass $\beta = \sqrt{\alpha - l^Tl}$. Dazu müssen wir sicherstellen, dass $\alpha - l^Tl > 0$.
		
		Wenn wir die Definition von $l$ verwenden und Umformen erhalten wir
		
		\begin{align*}
			\alpha - l^Tl &= \alpha - (P^{-1}a)^T(P^{-1}a) = \alpha - a^T (P^{-1})^TP^{-1}a \\
			&= \alpha - a^T (PP^T)^{-1}a = \alpha - a^T B^{-1}a
		\end{align*}
	
		\begin{align*}
			0 < \begin{pmatrix}
				-B^{-1}a\\
				1
			\end{pmatrix}^T
			\underbrace{\begin{pmatrix}
				B & a\\
				a^T & \alpha
			\end{pmatrix}}_{=A}
			\begin{pmatrix}
				-B^{-1}a\\
				1
			\end{pmatrix} =
			\alpha - a^T B^{-1} a
		\end{align*}
	
		Also können wir $\beta := \sqrt{\alpha - l^Tl}$ setzen. Dann gilt $l^Tl + \beta^2 = \alpha$.
		
		Definieren wir nun $L \in \mathbb{R}^{n\times n}$ durch
		
		\begin{align*}
			L = \begin{pmatrix}
				P & 0 \\
				l^T & \beta
			\end{pmatrix}
		\end{align*}
	
		Dann gilt
		
		\begin{align*}
			LL^T = \begin{pmatrix}
				P & 0 \\
				l^T & \beta
			\end{pmatrix} \begin{pmatrix}
				P^T & l \\
				0 & \beta
			\end{pmatrix} = \begin{pmatrix}
				PP^T & Pl \\
				l^TP^T & l^Tl+\beta^2
			\end{pmatrix} = \begin{pmatrix}
				PP^T & Pl \\
				(Pl)^T & l^Tl+\beta^2
			\end{pmatrix} = \begin{pmatrix}
				B & a \\
				a^T & \alpha
			\end{pmatrix} = A
		\end{align*}
	\end{itemize}
	
	
\end{proof}

\subsection*{Teilaufgabe 2b:}
Beweisen Sie, dass die Besetzungsstruktur der Cholesky-Zerlegung der Skyline-Matrix $A$ erhalten bleibt, d.h. dass auch die untere Dreiecksmatrix $L$ eine geeignete Bandstruktur aufweist.

\begin{proof}
	TODO Beweis 2b
\end{proof}

\section*{Aufgabe 3: Pseudocode für Cholesky-Zerlegung von Skyline-Matrizen}
Verwenden Sie den Cholesky-Algorithmus aus der Vorlesung. Entwerfen Sie jeweils einen Pseudocode, der für eine Skyline-Matrix:
\subsection{Teilaufgabe 3a:}
möglichst effizient die Struktur erkennt.

TODO Pseudocode

\subsection*{Teilaufgabe 3b:}
die Cholesky-Zerlegung berechnet.

TODO Pseudocode aufschreiben

\section*{Aufgabe 4: Aufwand des Algorithmus und Verhalten in Spezialfällen}
\subsection*{Teilaufgabe 4a:}
Sei $A \in \mathbb{R}^{n\times n}$ eine Skyline-Matrix. Welchen Aufwand haben Ihre Algorithmen aus Aufgabe 3 in Abhängigkeit von der Größe $n$ der Eingabedaten und Skyline-Indices $p_l = q_l$?

TODO Aufwand bestimmen

\subsection*{Teilaufgabe 4b:}
Betrachten Sie Matrizen mit den Besetzungsstrukturen

\begin{align*}
	\begin{pmatrix}
		* & * & * & * & *\\
		* & * &   &   &  \\
		* &   & * &   &  \\
		* &   &   & * &  \\
		* &   &   &   & *\\
	\end{pmatrix}
	\text{ bzw. }
	\begin{pmatrix}
	* &   &   &   & *\\
	  & * &   &   & *\\
	  &   & * &   & *\\
	  &   &   & * & *\\
	* & * & * & * & *\\
\end{pmatrix}
\end{align*}

Welche Besetzungsstruktur hat die Cholesky-Zerlegung für beide Matrizen? Was könnte man machen, um für Matrizen mit der ''linken'' Besetzungsstruktur die Cholesky-Zerlegung effizienter zu berechnen?

linke Matrix ist quasi vollbesetzt als Skyline-Matrix (mit Nulleinträgen!)

rechte Matrix hat p=q=0 für alle außer letzte Zeile dort p=q=n

effizientere berechnung der cholesky-Zerlegung durch ''spiegeln''. Also aus $A \in \mathbb{R}^{n\times n}$ mache $B \in \mathbb{R}^{n\times n}$ durch $B_{i,j} = A_{(n+1-i),(n+1-j)}$. Umkehrabbildung ist gleich (auch $(i, j) \mapsto (n+1-i, n+1-j)$). Dann erhält man eine Matrix mit der rechten Form, kann die cholesky-zerlegung $LL^T=B$ berechnen und erhält dann, dass $L$ gespiegelt (auch wieder gleich) eine untere Dreiecksmatrix mit $\hat{L}\hat{L}^T=A$.

\section*{Aufgabe 5: Implementierung des Algorithmus und empirische Aufwandsschätzung}
Implementieren Sie Ihren modifizierten Cholesky-Algorithmus in Python und weisen Sie empirisch nach, dass der Aufwand linear in $n$ wächst. Vergleichen Sie die Performance Ihrer Implementierung mit der Python-Funktion \texttt{scipy.linalg.cholesky}, wobei die Skyline-Matrix $A$ als vollbesetzte Matrix gespeichert ist.

TODO Anhang Python-Code (+ Grafik Performance?)


\end{document}
